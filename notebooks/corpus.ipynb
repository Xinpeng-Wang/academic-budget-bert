{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = load_dataset('bookcorpusopen', split=\"train\", cache_dir='/mounts/data/proj/xinpeng/huggingface/bookcorpusopen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookscorpusTextFormatting:\n",
    "    def __init__(self, books_path, output_filename, recursive=False):\n",
    "        self.books_path = books_path\n",
    "        self.recursive = recursive\n",
    "        self.output_filename = output_filename\n",
    "\n",
    "    # This puts one book per line\n",
    "    def merge(self):\n",
    "        with open(self.output_filename, mode=\"w\", newline=\"\\n\") as ofile:\n",
    "            for filename in glob.glob(self.books_path + \"/\" + \"*.txt\", recursive=True):\n",
    "                with open(filename, mode=\"r\", encoding=\"utf-8-sig\", newline=\"\\n\") as file:\n",
    "                    for line in file:\n",
    "                        if line.strip() != \"\":\n",
    "                            ofile.write(line.strip() + \" \")\n",
    "                ofile.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creat a book toy ####\n",
    "book_toy_path = '/mounts/data/proj/xinpeng/toy/book_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write huggingface book corpus to book toy ###\n",
    "with open (book_toy_path, 'w+', newline='\\n') as f:\n",
    "    for idx, i in enumerate(book_dataset):\n",
    "        doc_raw = i['text']\n",
    "        f.write(doc_raw)\n",
    "        if idx == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_toy_wrapper = open(book_toy_path, mode='r', newline='\\n')\n",
    "for i in book_toy_wrapper:\n",
    "    print(i)\n",
    "    break\n",
    "book_toy_wrapper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_budget = '/mounts/data/proj/xinpeng/budget/bookcorpus_one_article_per_line.txt'\n",
    "with open(book_budget, 'r') as f:\n",
    "    a=f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b, eps=1e-8):\n",
    "    return (a * b).sum(1) / (a.norm(dim=1) * b.norm(dim=1) + eps)\n",
    "\n",
    "\n",
    "def pearson_correlation(a, b, eps=1e-8):\n",
    "    return cosine_similarity(a - a.mean(1).unsqueeze(1),\n",
    "                             b - b.mean(1).unsqueeze(1), eps)\n",
    "\n",
    "\n",
    "def inter_class_relation(y_s, y_t):\n",
    "    return 1 - pearson_correlation(y_s, y_t).mean()\n",
    "\n",
    "\n",
    "def intra_class_relation(y_s, y_t):\n",
    "    return inter_class_relation(y_s.transpose(0, 1), y_t.transpose(0, 1))\n",
    "\n",
    "\n",
    "class DIST(nn.Module):\n",
    "    def __init__(self, beta=1.0, gamma=1.0):\n",
    "        super(DIST, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, z_s, z_t):\n",
    "        y_s = z_s.softmax(dim=1)\n",
    "        y_t = z_t.softmax(dim=1)\n",
    "        inter_loss = inter_class_relation(y_s, y_t)\n",
    "        intra_loss = intra_class_relation(y_s, y_t)\n",
    "        kd_loss = self.beta * inter_loss + self.gamma * intra_loss\n",
    "        return kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dist = DIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([32, 16, 128, 128])\n",
    "b = torch.rand([32, 16, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b, eps=1e-8):\n",
    "    return (a * b).sum(-1) / (a.norm(dim=-1) * b.norm(dim=-1) + eps)\n",
    "\n",
    "\n",
    "def pearson_correlation(a, b, eps=1e-8):\n",
    "    return cosine_similarity(a - a.mean(-1).unsqueeze(-1),\n",
    "                             b - b.mean(-1).unsqueeze(-1), eps)\n",
    "\n",
    "\n",
    "def inter_class_relation(y_s, y_t):\n",
    "    return 1 - pearson_correlation(y_s, y_t).mean()\n",
    "\n",
    "\n",
    "\n",
    "class DIST_ATT(nn.Module):\n",
    "    def __init__(self, beta=1.0, gamma=1.0):\n",
    "        super(DIST_ATT, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, z_s, z_t):\n",
    "        y_s = z_s.softmax(dim=-1)\n",
    "        y_t = z_t.softmax(dim=-1)\n",
    "        inter_token_1 = inter_class_relation(y_s, y_t)\n",
    "        inter_token_2 = inter_class_relation(y_s.transpose(2, 3), y_t.transpose(2, 3))\n",
    "        inter_head = inter_class_relation(y_s.transpose(1, 3), y_t.transpose(1, 3))\n",
    "        inter_sentence = inter_class_relation(y_s.transpose(0, 3), y_t.transpose(0, 3))\n",
    "        kd_loss = inter_token_1 + inter_token_2 + inter_head + inter_sentence\n",
    "        return kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=DIST_ATT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save teacher for finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dict={\n",
    "  \"add_nsp\": False,\n",
    "  \"async_worker\": True,\n",
    "  \"attention_dropout_checkpoint\": False,\n",
    "  \"current_run_id\": \"\",\n",
    "  \"data_loader_type\": \"dist\",\n",
    "  \"dataset_path\": \"/mounts/Users/student/xinpeng/data/budget/masked\",\n",
    "  \"deepspeed\": False,\n",
    "  \"deepspeed_config\": \"training-out/pretraining_experiment-/epoch1000000_step14998/deepspeed_config.json\",\n",
    "  \"deepspeed_transformer_kernel\": False,\n",
    "  \"do_validation\": False,\n",
    "  \"ds_config\": {\n",
    "    \"fp16\": {\n",
    "      \"enabled\": True,\n",
    "      \"hysteresis\": 2,\n",
    "      \"loss_scale\": 0,\n",
    "      \"loss_scale_window\": 1000,\n",
    "      \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"gradient_clipping\": 0.0,\n",
    "    \"steps_per_print\": 100,\n",
    "    \"train_batch_size\": 4096,\n",
    "    \"train_micro_batch_size_per_gpu\": 64,\n",
    "    \"wall_clock_breakdown\": False\n",
    "  },\n",
    "  \"early_exit_time_marker\": 24.0,\n",
    "  \"early_stop_eval_loss\": 6.0,\n",
    "  \"early_stop_time\": 180,\n",
    "  \"exp_start_marker\": 10749109.734136138,\n",
    "  \"finetune_checkpoint_at_end\": True,\n",
    "  \"fp16\": True,\n",
    "  \"fp16_backend\": \"ds\",\n",
    "  \"fp16_opt\": \"O2\",\n",
    "  \"gelu_checkpoint\": False,\n",
    "  \"gradient_accumulation_steps\": 8,\n",
    "  \"gradient_clipping\": 0.0,\n",
    "  \"job_name\": \"pretraining_experiment-\",\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"local_rank\": 0,\n",
    "  \"log_throughput_every\": 20,\n",
    "  \"lr\": 0.001,\n",
    "  \"max_predictions_per_seq\": 20,\n",
    "  \"max_steps\": 9223372036854775807,\n",
    "  \"max_steps_per_epoch\": 9223372036854775807,\n",
    "  \"model_config\": {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"encoder_ln_mode\": \"post-ln\",\n",
    "    \"fused_linear_layer\": False,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_type\": \"pytorch\",\n",
    "    \"layernorm_embedding\": True,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"sparse_mask_prediction\": True,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"vocab_size\": 30522,\n",
    "  },\n",
    "  \"model_type\": \"bert-mlm\",\n",
    "  \"no_nsp\": True,\n",
    "  \"normalize_invertible\": False,\n",
    "  \"num_epochs\": 1000000,\n",
    "  \"num_epochs_between_checkpoints\": 10000,\n",
    "  \"num_workers\": 4,\n",
    "  \"output_dir\": \"training-out\",\n",
    "  \"prescale_gradients\": False,\n",
    "  \"print_steps\": 100,\n",
    "  \"project_name\": \"budget-bert-pretraining\",\n",
    "  \"saved_model_path\": \"training-out/pretraining_experiment-/\",\n",
    "  \"scale_cnt_limit\": 100,\n",
    "  \"seed\": 42,\n",
    "  \"steps_per_print\": 100,\n",
    "  \"stochastic_mode\": False,\n",
    "  \"tokenizer_name\": \"bert-base-uncased\",\n",
    "  \"total_training_time\": 24.0,\n",
    "  \"train_batch_size\": 4096,\n",
    "  \"train_micro_batch_size_per_gpu\": 64,\n",
    "  \"use_early_stopping\": True,\n",
    "  \"validation_begin_proportion\": 0.05,\n",
    "  \"validation_end_proportion\": 0.01,\n",
    "  \"validation_epochs\": 3,\n",
    "  \"validation_epochs_begin\": 1,\n",
    "  \"validation_epochs_end\": 1,\n",
    "  \"validation_micro_batch\": 16,\n",
    "  \"vocab_size\": 30522,\n",
    "  \"wall_clock_breakdown\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/xinpeng/miniconda3/envs/budget/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from attrdict import AttrDict\n",
    "import os\n",
    "os.chdir('/mounts/Users/student/xinpeng/code/academic-budget-bert')\n",
    "from pretraining.base import BasePretrainModel, PretrainedBertConfig, BertForMaskedLM\n",
    "from pretraining.modeling import BertLMHeadModel\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "# from pretraining.utils import budget_to_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load('notebooks/data/batch_toy.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2022 21:02:29 - INFO - pretraining.base -   Loading default tokenizer bert-base-uncased\n",
      "07/28/2022 21:02:31 - INFO - pretraining.base -   Loading config from args\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2022 21:02:32 - INFO - pretraining.modeling -   Init BERT pretrain model\n",
      "07/28/2022 21:02:34 - INFO - pretraining.modeling -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_ln_mode\": \"post-ln\",\n",
      "  \"fused_linear_layer\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"hugging_face\": true,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"pytorch\",\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_mask_prediction\": false,\n",
      "  \"transformers_version\": \"4.21.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"useLN\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2022 21:02:34 - INFO - pretraining.modeling -   Init BERT pretrain model\n",
      "07/28/2022 21:02:36 - INFO - pretraining.modeling -   Loading model models/teachers/bert-base-uncased/pytorch_model.bin\n",
      "07/28/2022 21:02:37 - INFO - pretraining.modeling -   loading model...\n",
      "07/28/2022 21:02:37 - INFO - pretraining.modeling -   done!\n",
      "07/28/2022 21:02:37 - INFO - pretraining.modeling -   Weights of BertLMHeadModel not initialized from pretrained model: ['bert.encoder.FinalLayerNorm.weight', 'bert.encoder.FinalLayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "07/28/2022 21:02:37 - INFO - pretraining.modeling -   Weights from pretrained model not used in BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "/mounts/Users/student/xinpeng/miniconda3/envs/budget/lib/python3.7/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_args =AttrDict(dict)\n",
    "student = BasePretrainModel(model_args)\n",
    "student.network = BertLMHeadModel.from_pretrained_customized('models/teachers/bert-base-uncased', args=model_args)\n",
    "student.network.eval()\n",
    "student.network.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    attentions_teacher, qkv_teacher, prediction_score_teacher = \\\n",
    "                student.network(batch, output_attentions=True, output_qkv=True, output_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2022 17:24:54 - INFO - pretraining.modeling -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_ln_mode\": \"post-ln\",\n",
      "  \"fused_linear_layer\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"hugging_face\": true,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"pytorch\",\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_mask_prediction\": false,\n",
      "  \"transformers_version\": \"4.21.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"useLN\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2022 17:24:55 - INFO - pretraining.modeling -   Init BERT pretrain model\n",
      "07/28/2022 17:24:56 - INFO - pretraining.modeling -   Loading model models/teachers/bert-base-uncased/pytorch_model.bin\n",
      "07/28/2022 17:24:57 - INFO - pretraining.modeling -   loading model...\n",
      "07/28/2022 17:24:57 - INFO - pretraining.modeling -   done!\n",
      "07/28/2022 17:24:57 - INFO - pretraining.modeling -   Weights of BertLMHeadModel not initialized from pretrained model: ['bert.encoder.FinalLayerNorm.weight', 'bert.encoder.FinalLayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "07/28/2022 17:24:57 - INFO - pretraining.modeling -   Weights from pretrained model not used in BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "a=BertLMHeadModel.from_pretrained_customized('models/teachers/bert-base-uncased/', args=None)\n",
    "a.to(\"cuda\")\n",
    "a.eval()\n",
    "a(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1863321/2256387831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"bert_base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m student.save_weights(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcheckpoint_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training-out/bert_base/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mis_deepspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'student' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_id = f\"bert_base\"\n",
    "student.save_weights(\n",
    "    checkpoint_id=checkpoint_id,\n",
    "    output_dir=\"training-out/bert_base/\",\n",
    "    is_deepspeed=model_args.deepspeed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict=student.network.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = budget_to_huggingface(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=2\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task='rte',\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('budget')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c75a57cb9da59b0d18f2da6da20d740cfe4a073a1fd8c2c1274a6640ef1272b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
